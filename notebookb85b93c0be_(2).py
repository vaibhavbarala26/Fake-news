# -*- coding: utf-8 -*-
"""notebookb85b93c0be_(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_pAQfGXi9YgQIUk77CyGjRDLPRW2Gh7
"""

from google.colab import drive
drive.mount('/content/drive')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Run this in a cell first
!pip install --upgrade tqdm

# Then restart kernel and run your training

pip install evaluate

from datasets import DatasetDict, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification,TrainingArguments, Trainer
from datasets import load_dataset
import evaluate
from transformers import DataCollatorWithPadding
import torch

data_path = "/content/drive/MyDrive/WELFake_Dataset.csv"
DF = pd.read_csv(data_path)

DF.dropna()

DF.drop("Unnamed: 0" , inplace=True , axis=1)

DF["Content"] = DF["title"]+DF["text"]

DF.drop(["title" , "text"] , axis=1 , inplace=True)

DF.isna().sum()

DF.dropna(inplace=True)

DF.isna().sum()

import re

def clean(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

DF

# Count duplicate rows based only on the "Content" column
num_duplicates = DF["Content"].duplicated().sum()
print(f"üîç Number of duplicate content rows: {num_duplicates}")

DF = DF.drop_duplicates(subset="Content").reset_index(drop=True)

DF

DF["Content"] = DF["Content"].apply(clean)

# Count duplicate rows based only on the "Content" column
num_duplicates = DF["Content"].duplicated().sum()
print(f"üîç Number of duplicate content rows: {num_duplicates}")

DF = DF.drop_duplicates(subset="Content").reset_index(drop=True)

DF = DF[DF["Content"].str.strip() != ""]

DF

DF_dataset = Dataset.from_pandas(DF)

accuracy = evaluate.load("accuracy")
auc_score = evaluate.load("roc_auc")

def compute_metrics(eval_pred):
    # get predictions
    predictions, labels = eval_pred

    # apply softmax to get probabilities
    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1,
                                                                 keepdims=True)
    # use probabilities of the positive class for ROC AUC
    positive_class_probs = probabilities[:, 1]
    # compute auc
    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs,
                                     references=labels)['roc_auc'],3)

    # predict most probable class
    predicted_classes = np.argmax(predictions, axis=1)
    # compute accuracy
    acc = np.round(accuracy.compute(predictions=predicted_classes,
                                     references=labels)['accuracy'],3)

    return {"accuracy": acc, "auc": auc}

model_path = "google-bert/bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_path)

# load model with binary classification head
id2label = {0: "Fake", 1: "Real"}
label2id = {"Fake": 0, "Real": 1}
model = AutoModelForSequenceClassification.from_pretrained(model_path,
                                                           num_labels=2,
                                                           id2label=id2label,
                                                           label2id=label2id,)

# Freeze all
for param in model.bert.parameters():
    param.requires_grad = False

# Unfreeze last 2 encoder layers
for i in range(8, 12):
    for param in model.bert.encoder.layer[i].parameters():
        param.requires_grad = True

print(DF_dataset)
print(DF_dataset.column_names)

def preprocess_function(examples):
    texts = examples["Content"]
    # Ensure all elements are strings
    texts = [str(t) if t is not None else "" for t in texts]
    return tokenizer(texts, truncation=True, padding=True, max_length=512)

# preprocess all datasets
tokenized_data = DF_dataset.map(preprocess_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/my_bert_model",  # ‚úÖ Save checkpoints to Drive
    eval_strategy="epoch",                        # ‚úÖ Corrected key name
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    weight_decay=0.01,
    warmup_ratio=0.1,
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",              # ‚úÖ Matches lowercase metric
    greater_is_better=True,
    logging_steps=50,
    save_total_limit=2,
    disable_tqdm=False,
    report_to=None,
    fp16=True  # ‚úÖ (Optional: mixed-precision for faster training on GPU)
)

import transformers
print(f"Transformers version: {transformers.__version__}")

# 80% train, 20% test
train_test = tokenized_data.train_test_split(test_size=0.2, seed=42)

# Split the 20% test into half validation, half test => 10% val, 10% test
val_test = train_test['test'].train_test_split(test_size=0.5, seed=42)

# Combine all splits into a DatasetDict
final_dataset = DatasetDict({
    'train': train_test['train'],
    'validation': val_test['train'],
    'test': val_test['test']
})

len(final_dataset["validation"])

# Convert to sets of strings for easy comparison
train_texts = set(final_dataset["train"]["Content"])
val_texts = set(final_dataset["validation"]["Content"])

# Find common entries
overlap = train_texts.intersection(val_texts)
print(f"üîç Overlapping samples: {len(overlap)}")

from tqdm.auto import tqdm
print("‚úÖ tqdm is ready!")

# Quick test to see if progress bars work
from tqdm.auto import tqdm
import time

print("Testing progress bar...")
for i in tqdm(range(10), desc="Test"):
    time.sleep(0.1)
print("‚úÖ Progress bar working!")

from transformers import EarlyStoppingCallback
# Create final dataset

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=final_dataset["train"],
    eval_dataset=final_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # ‚úÖ Stops if no improvement for 2 evals
)


# Start training - should now show progress bars!
# print("üöÄ Starting training with progress bars...")
# trainer.train()



# ‚úÖ Final model save path in Drive
final_save_path = f"{training_args.output_dir}/final_model"

print("Saving model...")
trainer.save_model(final_save_path)
tokenizer.save_pretrained(final_save_path)

print("Training completed!")
print(f"Best model and tokenizer saved in: {final_save_path}")

# Optional: Evaluate on test set

final_dataset["test"]

# Evaluate on validation set (or test set if you have one)
eval_results = trainer.evaluate(eval_dataset=final_dataset["test"])

print("üìä Evaluation Results:")
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

y_true = final_dataset["test"]["label"]
predictions = trainer.predict(final_dataset["test"])
probs = np.exp(predictions.predictions) / np.exp(predictions.predictions).sum(-1, keepdims=True)
y_scores = probs[:, 1]
y_pred = np.argmax(predictions.predictions, axis=1)

# ROC Curve
fpr, tpr, _ = roc_curve(y_true, y_scores)
plt.plot(fpr, tpr, label=f"AUC = {auc(fpr, tpr):.2f}")
plt.plot([0, 1], [0, 1], linestyle='--')
plt.title("ROC Curve")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.grid()
plt.show()

# Confusion Matrix

cm = confusion_matrix(y_true, y_pred)
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Fake", "Real"]).plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_path = "/content/drive/MyDrive/my_bert_model/final_model"
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

model.eval()

text = "Iran-Israel War News Live Updates: 1,000 Indian students moved out of Tehran, to be brought back in 3 flightsIran-Israel War Today News Live Updates: Over 200 people were left injured in Israel after Iran targeted a hospital and residential structures in Tel Aviv as the two sides traded strikes for the eight day in a row."

def predict_label(text):
    import re  # Ensure re is imported in case it's not already

    # Clean the input (same as training)
    def clean(text):
        text = text.lower()
        text = re.sub(r'[^a-z0-9\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    text = clean(text)

    # Tokenize
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1).numpy()[0]  # Get first example

    predicted_class = np.argmax(probs)
    label_map = {0: "Fake", 1: "Real"}
    predicted_label = label_map[predicted_class]

    # Convert to percentage format
    fake_pct = probs[0] * 100
    real_pct = probs[1] * 100

    return {
        "label": predicted_label,
        "confidence": {
            "Fake": f"{fake_pct:.2f}%",
            "Real": f"{real_pct:.2f}%"
        }
    }

predict_label(text)

